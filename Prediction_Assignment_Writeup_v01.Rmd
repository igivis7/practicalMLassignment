---
title: "Prediction Assignment Writeup"
author: "Igor Isaev"
date: "10/29/2020"
output: 
  html_document:
    toc: yes
    keep_md: yes
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>

```{r setup, include=FALSE}
  library(knitr)
  opts_chunk$set( echo=TRUE, fig.path='figure/', cache=TRUE, fig.height=2.5, fig.width=5, fig.align='center')
  knitr::opts_chunk$set(echo = TRUE)
```
<!-- Rscript -e 'rmarkdown::render("Prediction_Assignment_Writeup_v01.Rmd")' -->

## Overview

Using fitness tracker devices it is now possible to collect a large amount of data about personal activity relatively inexpensively. These types of devices help people regularly quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

This project utilizes data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The data come from the source: http://groupware.les.inf.puc-rio.br/har. Thanks to these enthusiasts for collecting and providing this data.

The goal of this project is to predict the manner in which the participants did the exercises (the "classe" variable in the Training set), create a report describing which steps were taken to build the final model. And apply the final model to the Test data set to predict 20 different test cases.

The current report provides a short version of the code outputs and some intermediate steps are only described in words to keep the report barely short.


## Exploratory Analysis
Let`s load the necessary libraries, data, and have a look into it.

```{r results='hide', message=FALSE}
	library(caret)
	library(doParallel)
	pml_tes <- read.csv('pml-testing.csv', na.strings = c("NA", "#DIV/0!", ""))
	pml_tra <- read.csv('pml-training.csv', na.strings = c("NA", "#DIV/0!", ""))
```
The Training and Test data consist of 160 variables/columns each. 
The first 159 variable names are identical in both data sets, but the last is "classe" in the Training data set and "problem_id" in the Test data set. The "classe" we should predict and the "problem_id" is simply an observation number in the Test data set.  
Here are the last twelve data sets names: 
```{r }
	names(pml_tra)[-(1:148)]
	names(pml_tes)[-(1:148)]
```
Some of the variables in the Training set have a lot of NA values.  
The following code provides detailed information about the variables.
```{r results='hide', message=FALSE}
	n_names <- names(pml_tra)
	xc_names = NULL
	xc_val_length = NULL
	xc_uval_length = NULL
	xc_var_claa = NULL
	xc_nofna = NULL
	for(xc1 in 1:length(n_names)){
	  xc_names[xc1] = n_names[xc1]
	  xc_val_length[xc1] = length( pml_tra[, n_names[xc1]] )
	  xc_uval_length[xc1] = length( unique(pml_tra[, n_names[xc1]]) )
	  xc_var_claa[xc1] = class(pml_tra[, n_names[xc1]])
	  xc_nofna[xc1] = sum(is.na( pml_tra[, n_names[xc1]] ))
	}
	variables_summary = data.frame(VarName = xc_names, 
	                               Values = xc_val_length, 
	                               Unique = xc_uval_length,
	                               VarClass = xc_var_claa,
	                               N_of_NA = xc_nofna)
```
```{r}
	variables_summary[1:17,]
```
The table shows only the first 17 variables, but the main features are described by them:  

- there are variables of 'character' type that should be converted to 'factor' variables for analysis  
- there are also some variables of class 'logical', which consist only NA values  
- some numeric variables also have a lot of NAs (more than 90%)

The manual about random forest algorithm (the most preferable algorithm) says that it is not handling missing values, that is why the current strategy is to get rid of them.  
There are also some variables that are not related to the data collected directly from the sensors but represent only descriptive information.


## Data preparation and cleaning

```{r}
	set.seed(15)
	# indexes of variables that consist of only NA
	idx_var_discarded  <- c(5, 6, 14, 17, 26, 89, 92, 101, 127, 130, 139)
	# indexes of variables that consist NA in general
	idx_var_ineligible <- c(12:36, 50:59, 69:83, 87:101, 103:112, 125:139, 141:150)
	# indexes of descriptive variables (one may use them in the data analysis but it is not necessary)
	idx_unnecessary_vars <- c(1,2,3,4,5,6,7)
	#let`s convert our target variable into factor
	pml_tra[,160] <- as.factor(pml_tra[,160])
	# let`s create cleaned data set
	pml_tdc <- pml_tra[, -c(idx_var_discarded, idx_var_ineligible, idx_unnecessary_vars)]
	sum(is.na(pml_tdc))

	#To preform cross validation of models, we should split our data to training and testing subsets
	idxTestC = createDataPartition(pml_tdc$classe, p = 0.25, list = FALSE)
	pmlTestC = pml_tdc[ idxTestC, ]
	pmlTrainC = pml_tdc[ -idxTestC, ]
	dim(pmlTestC)
	dim(pmlTrainC)
```
## Models training and comparison
In order to obtain a reliable model, let`s compare a few algorithms 'rpart', 'rf', and 'gbm'.
For acceleration of the training process, we use 'doParallel' library to utilize all cores of the processor.

```{r}
	#Decision Tree
	cl <- makePSOCKcluster(8)
	registerDoParallel(cl)
	  model_DT <- train(classe~., data = pmlTrainC, method = "rpart")
	stopCluster(cl)

	model_DT

	prediction_DT_TestC <- predict(model_DT, newdata = pmlTestC)
	confusionMatrix(prediction_DT_TestC, pmlTestC$classe)

	#Generalized Boosted Model
	cl <- makePSOCKcluster(8)
	registerDoParallel(cl)
	  model_GBM <- train(classe~., data = pmlTrainC, method = "gbm", verbose = FALSE)
	stopCluster(cl)

	model_GBM

	prediction_GBM_TestC <- predict(model_GBM, newdata = pmlTestC)
	confusionMatrix(prediction_GBM_TestC, pmlTestC$classe)


	#Random Forest
	cl <- makePSOCKcluster(8)
	registerDoParallel(cl)
	  model_RF <- train(classe~., data = pmlTrainC, method = "rf")
	stopCluster(cl)

	model_RF

	prediction_RF_TestC <- predict(model_RF, newdata = pmlTestC)
	confusionMatrix(prediction_RF_TestC, pmlTestC$classe)

```
The obtained accuracies are Decision Tree: 0.4946 , Generalized Boosted Model: 0.9621, Random Forest: 0.9955.  
The best performance shows Random Forest algorithm. 

## Variable Importance studies
In order to avoid the overfitting problem, we may reduce the number of predictors. The applied criteria for the predictor`s selection is to keep as little as possible the most important variables, while the prediction accuracy is still high.

A list of variables importance is:
```{r}
	varImpRF <- varImp(model_RF)
	varImpRF <- varImpRF$importance[order(varImpRF$importance$Overall,decreasing = TRUE),,drop = FALSE]
	varImp_var_list <- rownames(varImpRF)
	varImp_val <- varImpRF$Overall
```

The tests on 18, 13, 9, 6, 5, 4, 3, 2, and 1 the most important variables resulted in the selection of the 6 most important variables. This choice was made because the model with the 6 most important variables would keep 30% of the importance of the total variables (sum(varImp_val)), it would consist of only 11% of predictors (6 out of 52) and still have an accuracy of 98.5%, which is only 1% less than the complete model.
```{r}
	varImp_var_list[1:6]
```

The final model is:
```{r}
	cl <- makePSOCKcluster(8)
	registerDoParallel(cl)
	  model_RF_final <- train(classe~., data = pmlTrainC[,c(varImp_var_list[1:6],"classe")], method = "rf")
	stopCluster(cl)

	model_RF_final

	prediction_RF_final_TestC <- predict(model_RF_final, newdata = pmlTestC)
	confusionMatrix(prediction_RF_final_TestC, pmlTestC$classe)
```


## Prediction of the Test data set
Let`s apply the final model to the Test data set and submit it to the Quiz part. 
```{r}
	prediction_RF_TestPML <- predict(model_RF_final, newdata = pml_tes)
	prediction_RF_TestPML
```